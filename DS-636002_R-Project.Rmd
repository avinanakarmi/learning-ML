---
title: 'R Project: Future sale prediction'
output:
  html_document:
    df_print: paged
---

# Prerequisites
install.packages("e1071")
install.packages("lightgbm")
install.packages("xgboost")
install.packages("Metrics")



```{r}
rm(list=ls())
setwd("/Users/avinanakarmi/school/DS-636/project")
library(xgboost)
library(Metrics)
library(lightgbm)
library(dplyr)
library(e1071)
library(ggplot2)
library(tidyr)
library(tidyverse)
```


# Load data
```{r}
item_categories = read.csv("./data/item_categories.csv")
items = read.csv("./data/items.csv")
train_data =read.csv("./data/sales_train.csv")
shops = read.csv("./data/shops.csv")
test = read.csv("./data/test.csv")
```

# Examine data
```{r}
str(item_categories)
str(items)
str(shops)
str(train_data)
```


# Data aggregation
```{r}
item_info <- merge(items, item_categories, by = "item_category_id")
shop_info <- merge(train_data, shops, by = "shop_id")
training_data <- merge(item_info, shop_info, by="item_id")
# clean up memory
rm(item_info, shop_info, train_data, shops, item_categories)
```

# Data cleaning
```{r}
d1 <- training_data[complete.cases(training_data),]
# remove invalid sale info
d1$item_cnt_day <- abs(d1$item_cnt_day)
# remove data with invalid price info
data <- d1[d1$item_price >= 0, ]
data$date <- as.Date(data$date, format = "%d.%m.%Y")
rm(training_data, d1)
```

# Exploratory analysis
```{r}
# Over all data
paste("Training data spans from ", min(as.Date(data$date, format = "%d.%m.%Y")), "to ", max(as.Date(data$date, format = "%d.%m.%Y")))
```

## Monthly sale data
```{r}
month_names <- c("January", "February", "March", "April", "May", "June",
                 "July", "August", "September", "October", "November", "December")
data <- data %>%
  mutate(month_name = factor(month_names[date_block_num %% 12 + 1], levels = month_names))

ggplot(data=data) +
  geom_bar(aes(x=date_block_num, fill = month_name)) +
  labs(title = "Total sales each month")

data <- subset(data, select = -c(month_name))
rm(month_names)
```

**Conclusion**
 * _Sales are highest in December_
 * _Sales are generally lower in May and June_

## Items by category sold each month
```{r}
# Filter the data to include only rows where date_block_num = 0 or 12
filtered_data <- data %>%
  filter(date_block_num == 0 | date_block_num == 12)

# Create the plot using the filtered data
ggplot(filtered_data) +
  geom_bar(aes(x = item_category_id)) +
  facet_wrap(~date_block_num, nrow=2, ncol=1, scale="free") +
  theme(axis.text.x = element_text(size = 4)) +
  labs(title = "Total sales by item category for month", 
       subtitle = paste("Months", 0, "and", 12))

# Filter the data to include only rows where date_block_num = 5 or 17
filtered_data <- data %>%
  filter(date_block_num == 11 | date_block_num == 23)

# Create the plot using the filtered data
ggplot(filtered_data) +
  geom_bar(aes(x = item_category_id)) +
  facet_wrap(~date_block_num, nrow=2, ncol=1, scale="free") +
  theme(axis.text.x = element_text(size = 4)) + 
  labs(title = "Total sales by item category for month", 
       subtitle = paste("Months", 11, "and", 23))
```

**Conclusion**
 * _Games with category IDs between 18 to 21 are sold in higher quantity in December_

## Sales and price relation
```{r}
# Remove outlier prices
price_summary <- fivenum(data$item_price)
Q1 <- price_summary[2]
Q3 <- price_summary[4]
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
filtered_df <- data[data$item_price >= lower_bound & data$item_price <= upper_bound, ]
 
ggplot(data=filtered_df) +
  geom_point(aes(x=item_price, y=item_cnt_day))

rm(filtered_df, Q1, Q3, IQR, lower_bound, upper_bound, price_summary)
```


# Training Model
```{r}
# splitting train data
# Select feature columns
features <- c("item_id", "shop_id")
X <- data[, features]

# Select target column
y <- data$item_cnt_day

# Set a random seed for reproducibility
set.seed(1)

# Calculate the size of the training set (80% of the data)
train_size <- floor(0.8 * nrow(data))

# Randomly sample indices for the training set
train_indices <- sample(seq_len(nrow(data)), size = train_size)

# Create validation indices as the set difference between all indices and the training indices
valid_indices <- setdiff(seq_len(nrow(data)), train_indices)

# Split the data and target into training and validation sets using the indices
x_train <- X[train_indices, ]
x_valid <- X[valid_indices, ]
y_train <- y[train_indices]
y_valid <- y[valid_indices]
```

## XGBoost
```{r}
set.seed(1)

params <- list (
  verbosity = 0,
  n_estimators = list(100, 500, 1000, 1500, 2000, 2500, 3000),
  learning_rate = list(1e-5, 1e-4, 1e-3, 1e-2),
  colsample_bytree = list(0.5,0.6,0.7,0.8,0.9,1.0),
  subsample = list(0.6,0.7,0.8,1.0),
  gamma = 1,
  random_state = 1 
)
model <- xgboost(params = params, data = as.matrix(x_train), label = y_train, 
                 objective = "reg:squarederror", nrounds = 1000, early_stopping_rounds = 100)

predictions <- predict(model, as.matrix(x_valid))
predictions <- round(predictions)

msq <- mse(y_valid, predictions)

cat(sprintf("MSQ - %.4f\n", msq))
```

## LightGBM
```{r}
# Convert data and labels to LightGBM dataset format
dtrain <- lgb.Dataset(data = as.matrix(x_train), label = y_train)
dvalid <- lgb.Dataset.create.valid(dtrain, data = as.matrix(x_valid), label = y_valid)

# Set up parameters for the LightGBM model
params <- list(
  colsample_bytree = 0.75,
  metric = 'rmse',
  min_data_in_leaf = 128, 
  subsample = 0.75, 
  learning_rate = 0.01, 
  objective = 'regression', 
  bagging_seed = 128, 
  num_leaves = 128,
  bagging_freq = 1,
  seed = 1204
)

# Train the LightGBM model
model <- lgb.train(params = params,
                   data = dtrain,
                   valids = list(valid = dvalid),
                   nrounds = 1000,
                   early_stopping_rounds = 100)

# Make predictions on the validation set
predictions <- predict(model, as.matrix(x_valid))

# Round the predictions to the nearest integers
predictions <- round(predictions)

# Calculate mean squared error
msq <- mean((predictions - y_valid)^2)

# Print the mean squared error
cat(sprintf("MSQ - %.4f", msq))

```

** Algorithm comparison **
* MSE for XGBoost is 12.8815 and that for LightGBM is 12.1950
* LightGBM has lower MSE, so we choose LightGBM to make necessary prediction

# Prediction
```{r}
features <- c("item_id", "shop_id")
X <- test[, features]
test$item_cnt_day <- predict(model, as.matrix(X))
test
```
